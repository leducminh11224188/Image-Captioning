{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4845244,"sourceType":"datasetVersion","datasetId":2808179}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!rm -rf /kaggle/working/vocab.pkl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T07:46:38.474796Z","iopub.execute_input":"2026-02-12T07:46:38.475918Z","iopub.status.idle":"2026-02-12T07:46:38.609653Z","shell.execute_reply.started":"2026-02-12T07:46:38.475877Z","shell.execute_reply":"2026-02-12T07:46:38.608607Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.optim.lr_scheduler import LambdaLR\nfrom PIL import Image\nimport pandas as pd\nimport numpy as np\nimport os\nimport re\nimport math\nfrom tqdm import tqdm\nimport gc","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-12T07:46:47.984171Z","iopub.execute_input":"2026-02-12T07:46:47.984730Z","iopub.status.idle":"2026-02-12T07:46:47.991689Z","shell.execute_reply.started":"2026-02-12T07:46:47.984691Z","shell.execute_reply":"2026-02-12T07:46:47.990785Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Device: {device}\")\n\n# ƒê∆∞·ªùng d·∫´n Kaggle\nIMG_DIR = \"/kaggle/input/flickr30k/flickr30k_images\"\nCAPTION_FILE = \"/kaggle/input/flickr30k/captions.txt\"\nFEATURE_DIR = \"/kaggle/working/features\"\n\n# Tham s·ªë Model - TRANSFORMER\nEMBED_SIZE = 512          # Embedding dimension\nHIDDEN_SIZE = 512         # Hidden dimension (d_model trong Transformer)\nNUM_LAYERS = 3            # S·ªë Transformer decoder layers\nNUM_HEADS = 8             # S·ªë attention heads\nDROPOUT = 0.3             # Dropout rate\nFFN_DIM = 2048            # Feed-forward network dimension (4x hidden)\n\n# Tham s·ªë Training\nBATCH_SIZE = 32           # Batch size\nNUM_WORKERS = 2           # DataLoader workers\nNUM_EPOCHS = 15           # S·ªë epochs\nWARMUP_STEPS = 4000       # Warmup steps cho learning rate\nMAX_LR = 1e-4             # Max learning rate\n\n# T·∫°o th∆∞ m·ª•c\nos.makedirs(FEATURE_DIR, exist_ok=True)\n\nprint(f\"Configuration loaded\")\nprint(f\"   - HIDDEN_SIZE: {HIDDEN_SIZE}\")\nprint(f\"   - NUM_LAYERS: {NUM_LAYERS}\")\nprint(f\"   - NUM_HEADS: {NUM_HEADS}\")\nprint(f\"   - NUM_EPOCHS: {NUM_EPOCHS}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T07:46:49.471704Z","iopub.execute_input":"2026-02-12T07:46:49.472726Z","iopub.status.idle":"2026-02-12T07:46:49.487464Z","shell.execute_reply.started":"2026-02-12T07:46:49.472693Z","shell.execute_reply":"2026-02-12T07:46:49.486640Z"}},"outputs":[{"name":"stdout","text":"Device: cpu\nConfiguration loaded\n   - HIDDEN_SIZE: 512\n   - NUM_LAYERS: 3\n   - NUM_HEADS: 8\n   - NUM_EPOCHS: 15\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"class Vocabulary:\n    \"\"\"\n    X√¢y d·ª±ng t·ª´ ƒëi·ªÉn t·ª´ captions\n    \"\"\"\n    def __init__(self, freq_threshold):\n        self.itos = {0: \"<pad>\", 1: \"<start>\", 2: \"<end>\", 3: \"<unk>\"}\n        self.stoi = {\"<pad>\": 0, \"<start>\": 1, \"<end>\": 2, \"<unk>\": 3}\n        self.freq_threshold = freq_threshold\n\n    def __len__(self):\n        return len(self.itos)\n\n    @staticmethod\n    def tokenizer_eng(text):\n        \"\"\"\n        Tokenize English text\n        FIX: S·ª≠ d·ª•ng regex ƒë·ªÉ lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát\n        \"\"\"\n        text = str(text).lower()\n        text = re.sub(r'[^a-z ]', '', text)  # Ch·ªâ gi·ªØ ch·ªØ c√°i v√† kho·∫£ng tr·∫Øng\n        return text.split()\n\n    def build_vocabulary(self, sentence_list):\n        \"\"\"X√¢y d·ª±ng vocabulary t·ª´ list captions\"\"\"\n        frequencies = {}\n        idx = 4\n\n        for sentence in sentence_list:\n            for word in self.tokenizer_eng(sentence):\n                frequencies[word] = frequencies.get(word, 0) + 1\n                if frequencies[word] == self.freq_threshold:\n                    self.stoi[word] = idx\n                    self.itos[idx] = word\n                    idx += 1\n\n    def numericalize(self, text):\n        \"\"\"Chuy·ªÉn text th√†nh list of indices\"\"\"\n        tokenized_text = self.tokenizer_eng(text)\n        return [self.stoi.get(token, self.stoi[\"<unk>\"]) for token in tokenized_text]\n\nprint(\"Class Vocabulary ƒë√£ xong\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T07:46:51.296676Z","iopub.execute_input":"2026-02-12T07:46:51.297120Z","iopub.status.idle":"2026-02-12T07:46:51.311534Z","shell.execute_reply.started":"2026-02-12T07:46:51.297081Z","shell.execute_reply":"2026-02-12T07:46:51.310535Z"}},"outputs":[{"name":"stdout","text":"Class Vocabulary ƒë√£ xong\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Load v√† x·ª≠ l√Ω d·ªØ li·ªáu\nprint(\"LOADING & PROCESSING DATA\")\nprint(\"=\"*80)\n\ndf = pd.read_csv(CAPTION_FILE)\nprint(f\"Loaded {len(df)} captions\")\nprint(f\"Columns: {df.columns.tolist()}\")\nprint(f\"\\nSample data:\")\nprint(df.head())\n\n# X√¢y d·ª±ng vocabulary\nvocab = Vocabulary(freq_threshold=3)\nvocab.build_vocabulary(df['comment'].tolist())\nprint(f\"\\nBuilt vocabulary with {len(vocab)} tokens\")\nprint(f\"Special tokens: <pad>={vocab.stoi['<pad>']}, \"\n      f\"<start>={vocab.stoi['<start>']}, \"\n      f\"<end>={vocab.stoi['<end>']}, \"\n      f\"<unk>={vocab.stoi['<unk>']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T07:46:52.675814Z","iopub.execute_input":"2026-02-12T07:46:52.676716Z","iopub.status.idle":"2026-02-12T07:46:54.100122Z","shell.execute_reply.started":"2026-02-12T07:46:52.676684Z","shell.execute_reply":"2026-02-12T07:46:54.099062Z"}},"outputs":[{"name":"stdout","text":"LOADING & PROCESSING DATA\n================================================================================\nLoaded 158915 captions\nColumns: ['image_name', 'comment_number', 'comment']\n\nSample data:\n       image_name  comment_number  \\\n0  1000092795.jpg               0   \n1  1000092795.jpg               1   \n2  1000092795.jpg               2   \n3  1000092795.jpg               3   \n4  1000092795.jpg               4   \n\n                                             comment  \n0  Two young guys with shaggy hair look at their ...  \n1  Two young  White males are outside near many b...  \n2   Two men in green shirts are standing in a yard .  \n3       A man in a blue shirt standing in a garden .  \n4            Two friends enjoy time spent together .  \n\nBuilt vocabulary with 9964 tokens\nSpecial tokens: <pad>=0, <start>=1, <end>=2, <unk>=3\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# ===================================================================\n# SAVE VOCAB - CH·∫†Y NGAY TRONG NOTEBOOK HI·ªÜN T·∫†I (KH√îNG C·∫¶N TRAIN L·∫†I)\n# ===================================================================\n\nimport pickle\nfrom IPython.display import FileLink\n\n# Ki·ªÉm tra vocab c√≥ t·ªìn t·∫°i kh√¥ng\nprint(f\"‚úÖ Vocab ƒë√£ t·ªìn t·∫°i v·ªõi {len(vocab)} tokens\")\n\n# Save vocab\nwith open('/kaggle/working/vocab.pkl', 'wb') as f:\n    pickle.dump(vocab, f)\n\nprint(\"‚úÖ ƒê√£ save vocab.pkl\")\n\n# Download link\nprint(\"\\nüì• Click ƒë·ªÉ t·∫£i xu·ªëng:\")\ndisplay(FileLink('/kaggle/working/vocab.pkl'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T07:46:54.978811Z","iopub.execute_input":"2026-02-12T07:46:54.979639Z","iopub.status.idle":"2026-02-12T07:46:54.993015Z","shell.execute_reply.started":"2026-02-12T07:46:54.979608Z","shell.execute_reply":"2026-02-12T07:46:54.992048Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Vocab ƒë√£ t·ªìn t·∫°i v·ªõi 9964 tokens\n‚úÖ ƒê√£ save vocab.pkl\n\nüì• Click ƒë·ªÉ t·∫£i xu·ªëng:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/vocab.pkl","text/html":"<a href='/kaggle/working/vocab.pkl' target='_blank'>/kaggle/working/vocab.pkl</a><br>"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"class EncoderCNN(nn.Module):\n    \"\"\"\n    Encoder: EfficientNet-B0 ƒë·ªÉ extract visual features\n    Output: (Batch, 49, 1280) - 49 spatial locations v·ªõi 1280-dim features\n    \"\"\"\n    def __init__(self):\n        super(EncoderCNN, self).__init__()\n        backbone = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n        self.features = backbone.features\n        \n        # Freeze encoder ƒë·ªÉ ti·∫øt ki·ªám memory v√† tr√°nh overfitting\n        for param in self.features.parameters():\n            param.requires_grad = False\n        \n        self.features.eval()\n            \n    def forward(self, images):\n        \"\"\"\n        Args:\n            images: (B, 3, 224, 224)\n        Returns:\n            features: (B, 49, 1280)\n        \"\"\"\n        with torch.no_grad():\n            features = self.features(images)  # (B, 1280, 7, 7)\n        \n        # Reshape to (B, 49, 1280)\n        features = features.permute(0, 2, 3, 1)  # (B, 7, 7, 1280)\n        features = features.view(features.size(0), -1, features.size(3))  # (B, 49, 1280)\n        return features\n\nprint(\"Class Encoder ƒë√£ xong\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T15:52:14.757133Z","iopub.execute_input":"2026-02-10T15:52:14.757434Z","iopub.status.idle":"2026-02-10T15:52:14.763556Z","shell.execute_reply.started":"2026-02-10T15:52:14.757402Z","shell.execute_reply":"2026-02-10T15:52:14.762817Z"}},"outputs":[{"name":"stdout","text":"Class Encoder ƒë√£ xong\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"encoder = EncoderCNN().to(device)\nprint(\"\\n\" + \"=\"*80)\nprint(\"ENCODER ARCHITECTURE\")\nprint(\"=\"*80)\nprint(\"EfficientNet-B0 Encoder initialized\")\nprint(\"Output shape: (Batch, 49, 1280)\")\nprint(\"Parameters: Frozen (no training)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T15:52:15.314020Z","iopub.execute_input":"2026-02-10T15:52:15.314681Z","iopub.status.idle":"2026-02-10T15:52:15.972906Z","shell.execute_reply.started":"2026-02-10T15:52:15.314655Z","shell.execute_reply":"2026-02-10T15:52:15.972228Z"}},"outputs":[{"name":"stdout","text":"Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20.5M/20.5M [00:00<00:00, 190MB/s]\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nENCODER ARCHITECTURE\n================================================================================\nEfficientNet-B0 Encoder initialized\nOutput shape: (Batch, 49, 1280)\nParameters: Frozen (no training)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\ndef extract_features_smart():\n    \"\"\"\n    Extract features t·ª´ images v√† l∆∞u v√†o disk\n    Ch·ªâ ch·∫°y n·∫øu ch∆∞a c√≥ features\n    \"\"\"\n    # Ki·ªÉm tra xem ƒë√£ extract ch∆∞a\n    if os.path.exists(FEATURE_DIR):\n        num_files = len([f for f in os.listdir(FEATURE_DIR) if f.endswith('.npy')])\n        if num_files > 30000:  # Flickr30k c√≥ ~31,783 ·∫£nh\n            print(f\"\\n‚úÖ Found {num_files} features in {FEATURE_DIR}\")\n            print(\"   Skipping feature extraction...\")\n            return\n    \n    print(f\"\\nFeature directory empty or incomplete\")\n    print(\"Starting feature extraction...\")\n    \n    unique_images = df['image_name'].unique()\n    encoder.eval()\n    \n    batch_size = 32\n    total_batches = (len(unique_images) + batch_size - 1) // batch_size\n    \n    with torch.no_grad():\n        for i in tqdm(range(0, len(unique_images), batch_size), \n                     desc=\"Extracting features\", total=total_batches):\n            batch_imgs_paths = unique_images[i : i + batch_size]\n            img_tensors = []\n            valid_paths = []\n            \n            for img_name in batch_imgs_paths:\n                img_path = os.path.join(IMG_DIR, img_name)\n                try:\n                    image = Image.open(img_path).convert(\"RGB\")\n                    image = transform(image)\n                    img_tensors.append(image)\n                    valid_paths.append(img_name)\n                except Exception as e:\n                    continue\n            \n            if not img_tensors:\n                continue\n            \n            batch_input = torch.stack(img_tensors).to(device)\n            features = encoder(batch_input)\n            features_np = features.cpu().numpy()\n            \n            for img_name, feature in zip(valid_paths, features_np):\n                save_path = os.path.join(FEATURE_DIR, img_name + \".npy\")\n                np.save(save_path, feature)\n            \n            # Clear memory\n            del batch_input, features, features_np\n            \n        gc.collect()\n        torch.cuda.empty_cache()\n    \n    num_saved = len([f for f in os.listdir(FEATURE_DIR) if f.endswith('.npy')])\n    print(f\"Saved {num_saved} feature files to {FEATURE_DIR}\")\n\n\n# Extract features\nextract_features_smart()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T15:52:17.599315Z","iopub.execute_input":"2026-02-10T15:52:17.599916Z","iopub.status.idle":"2026-02-10T15:52:17.628934Z","shell.execute_reply.started":"2026-02-10T15:52:17.599889Z","shell.execute_reply":"2026-02-10T15:52:17.628328Z"}},"outputs":[{"name":"stdout","text":"\n‚úÖ Found 31783 features in /kaggle/working/features\n   Skipping feature extraction...\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"class FlickrDataset(Dataset):\n    \"\"\"Dataset class ƒë·ªÉ load features v√† captions\"\"\"\n    def __init__(self, df, feature_dir, vocab):\n        self.df = df\n        self.feature_dir = feature_dir\n        self.vocab = vocab\n        self.imgs = df[\"image_name\"]\n        self.captions = df[\"comment\"]\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n        caption = self.captions.iloc[index]\n        img_id = self.imgs.iloc[index]\n\n        # Load feature\n        feature_path = os.path.join(self.feature_dir, img_id + \".npy\")\n        features = np.load(feature_path)  # (49, 1280)\n        \n        # Numericalize caption\n        numericalized_caption = [self.vocab.stoi[\"<start>\"]]\n        numericalized_caption += self.vocab.numericalize(caption)\n        numericalized_caption.append(self.vocab.stoi[\"<end>\"])\n\n        return torch.tensor(features, dtype=torch.float32), torch.tensor(numericalized_caption)\n\n\nclass MyCollate:\n    \"\"\"Collate function ƒë·ªÉ padding captions trong batch\"\"\"\n    def __init__(self, pad_idx):\n        self.pad_idx = pad_idx\n\n    def __call__(self, batch):\n        features = torch.stack([item[0] for item in batch], dim=0)  # (B, 49, 1280)\n        captions = [item[1] for item in batch]\n        \n        # Padding captions\n        targets = torch.nn.utils.rnn.pad_sequence(\n            captions, batch_first=True, padding_value=self.pad_idx\n        )\n\n        return features, targets\n\n\n# Chia train/val\ntrain_size = int(0.9 * len(df))\ntrain_df = df.iloc[:train_size].reset_index(drop=True)\nval_df = df.iloc[train_size:].reset_index(drop=True)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"CREATING DATALOADERS\")\nprint(\"=\"*80)\nprint(f\"Train samples: {len(train_df)}\")\nprint(f\"Val samples: {len(val_df)}\")\n\npad_idx = vocab.stoi[\"<pad>\"]\n\ntrain_dataset = FlickrDataset(train_df, FEATURE_DIR, vocab)\nval_dataset = FlickrDataset(val_df, FEATURE_DIR, vocab)\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    collate_fn=MyCollate(pad_idx),\n    num_workers=NUM_WORKERS,\n    pin_memory=True\n)\n\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    collate_fn=MyCollate(pad_idx),\n    num_workers=NUM_WORKERS,\n    pin_memory=True\n)\n\nprint(f\"Train batches: {len(train_loader)}\")\nprint(f\"Val batches: {len(val_loader)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T15:52:53.437119Z","iopub.execute_input":"2026-02-10T15:52:53.437844Z","iopub.status.idle":"2026-02-10T15:52:53.452381Z","shell.execute_reply.started":"2026-02-10T15:52:53.437817Z","shell.execute_reply":"2026-02-10T15:52:53.451627Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nCREATING DATALOADERS\n================================================================================\nTrain samples: 143023\nVal samples: 15892\nTrain batches: 4470\nVal batches: 497\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    \"\"\"Positional encoding cho Transformer\"\"\"\n    def __init__(self, d_model, max_len=100):\n        super(PositionalEncoding, self).__init__()\n        \n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        \n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n        \n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        return x + self.pe[:, :x.size(1), :]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T15:52:38.980545Z","iopub.execute_input":"2026-02-10T15:52:38.981237Z","iopub.status.idle":"2026-02-10T15:52:38.987132Z","shell.execute_reply.started":"2026-02-10T15:52:38.981208Z","shell.execute_reply":"2026-02-10T15:52:38.986334Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class TransformerDecoder(nn.Module):\n    \"\"\"\n    Transformer Decoder cho Image Captioning\n    \n    Architecture:\n    1. Embedding layer cho captions\n    2. Positional encoding\n    3. Multi-layer Transformer decoder\n    4. Output projection\n    \"\"\"\n    \n    def __init__(self, vocab_size, embed_size=512, hidden_size=512, \n                 num_layers=3, num_heads=8, dropout=0.3):\n        super(TransformerDecoder, self).__init__()\n        \n        self.vocab_size = vocab_size\n        self.embed_size = embed_size\n        self.hidden_size = hidden_size\n        \n        # 1. Embedding\n        self.embed = nn.Embedding(vocab_size, embed_size)\n        self.pos_encoder = PositionalEncoding(embed_size)\n        self.embed_dropout = nn.Dropout(dropout)\n        \n        # 2. Project image features (1280 -> hidden_size)\n        self.feature_proj = nn.Linear(1280, hidden_size)\n        \n        # 3. Transformer Decoder\n        decoder_layer = nn.TransformerDecoderLayer(\n            d_model=hidden_size,\n            nhead=num_heads,\n            dim_feedforward=hidden_size * 4,\n            dropout=dropout,\n            activation='gelu',\n            batch_first=True,\n            norm_first=True  # Pre-norm (better for deep models)\n        )\n        \n        self.transformer_decoder = nn.TransformerDecoder(\n            decoder_layer,\n            num_layers=num_layers\n        )\n        \n        # 4. Output\n        self.fc_out = nn.Linear(hidden_size, vocab_size)\n        self.dropout = nn.Dropout(dropout)\n        \n        # Initialize weights\n        self._init_weights()\n    \n    def _init_weights(self):\n        \"\"\"Xavier uniform initialization\"\"\"\n        for p in self.parameters():\n            if p.dim() > 1:\n                nn.init.xavier_uniform_(p)\n    \n    def generate_square_subsequent_mask(self, sz):\n        \"\"\"\n        Causal mask ƒë·ªÉ decoder kh√¥ng nh√¨n th·∫•y future tokens\n        \"\"\"\n        mask = torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n        return mask\n    \n    def forward(self, features, captions):\n        \"\"\"\n        Args:\n            features: (B, 49, 1280) - Image features\n            captions: (B, seq_len) - Caption tokens\n        \n        Returns:\n            logits: (B, seq_len, vocab_size)\n        \"\"\"\n        batch_size = features.size(0)\n        seq_len = captions.size(1)\n        \n        # 1. Project image features\n        memory = self.feature_proj(features)  # (B, 49, hidden)\n        \n        # 2. Embed captions\n        tgt = self.embed(captions) * math.sqrt(self.embed_size)\n        tgt = self.pos_encoder(tgt)\n        tgt = self.embed_dropout(tgt)  # (B, seq_len, embed)\n        \n        # 3. Create causal mask\n        tgt_mask = self.generate_square_subsequent_mask(seq_len).to(captions.device)\n        \n        # 4. Transformer decoder\n        output = self.transformer_decoder(\n            tgt=tgt,\n            memory=memory,\n            tgt_mask=tgt_mask\n        )  # (B, seq_len, hidden)\n        \n        # 5. Output projection\n        output = self.dropout(output)\n        logits = self.fc_out(output)  # (B, seq_len, vocab_size)\n        \n        return logits\n\n\n# Kh·ªüi t·∫°o model\nmodel = TransformerDecoder(\n    vocab_size=len(vocab),\n    embed_size=EMBED_SIZE,\n    hidden_size=HIDDEN_SIZE,\n    num_layers=NUM_LAYERS,\n    num_heads=NUM_HEADS,\n    dropout=DROPOUT\n).to(device)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"TRANSFORMER DECODER ARCHITECTURE\")\nprint(\"=\"*80)\nprint(f\"Vocab size: {len(vocab)}\")\nprint(f\"Embed size: {EMBED_SIZE}\")\nprint(f\"Hidden size: {HIDDEN_SIZE}\")\nprint(f\"Num layers: {NUM_LAYERS}\")\nprint(f\"Num heads: {NUM_HEADS}\")\nprint(f\"Dropout: {DROPOUT}\")\n\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"Total parameters: {total_params:,}\")\nprint(f\"Trainable parameters: {trainable_params:,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T15:53:00.109392Z","iopub.execute_input":"2026-02-10T15:53:00.109975Z","iopub.status.idle":"2026-02-10T15:53:00.407435Z","shell.execute_reply.started":"2026-02-10T15:53:00.109946Z","shell.execute_reply":"2026-02-10T15:53:00.406647Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nTRANSFORMER DECODER ARCHITECTURE\n================================================================================\nVocab size: 9964\nEmbed size: 512\nHidden size: 512\nNum layers: 3\nNum heads: 8\nDropout: 0.3\nTotal parameters: 23,481,068\nTrainable parameters: 23,481,068\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# Loss function\ncriterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n\n# Optimizer\noptimizer = optim.AdamW(\n    model.parameters(),\n    lr=MAX_LR,\n    betas=(0.9, 0.98),\n    eps=1e-9,\n    weight_decay=0.01\n)\n\n# Learning rate scheduler v·ªõi warmup\ndef lr_lambda(step):\n    \"\"\"Warmup + inverse sqrt decay\"\"\"\n    if step == 0:\n        return 0\n    if step < WARMUP_STEPS:\n        return step / WARMUP_STEPS\n    return (WARMUP_STEPS ** 0.5) * (step ** -0.5)\n\nscheduler = LambdaLR(optimizer, lr_lambda)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"TRAINING CONFIGURATION\")\nprint(\"=\"*80)\nprint(f\"Optimizer: AdamW\")\nprint(f\"Learning rate: {MAX_LR} (with warmup)\")\nprint(f\"Warmup steps: {WARMUP_STEPS}\")\nprint(f\"Batch size: {BATCH_SIZE}\")\nprint(f\"Num epochs: {NUM_EPOCHS}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T15:53:04.031700Z","iopub.execute_input":"2026-02-10T15:53:04.032252Z","iopub.status.idle":"2026-02-10T15:53:04.038488Z","shell.execute_reply.started":"2026-02-10T15:53:04.032222Z","shell.execute_reply":"2026-02-10T15:53:04.037800Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nTRAINING CONFIGURATION\n================================================================================\nOptimizer: AdamW\nLearning rate: 0.0001 (with warmup)\nWarmup steps: 4000\nBatch size: 32\nNum epochs: 15\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"def train_one_epoch(model, train_loader, criterion, optimizer, scheduler, epoch):\n    \"\"\"Train 1 epoch\"\"\"\n    model.train()\n    total_loss = 0\n    \n    loop = tqdm(train_loader, desc=f\"Epoch {epoch}/{NUM_EPOCHS} [TRAIN]\", leave=True)\n    \n    for features, captions in loop:\n        features = features.to(device)  # (B, 49, 1280)\n        captions = captions.to(device)  # (B, seq_len)\n        \n        # Forward\n        outputs = model(features, captions[:, :-1])  # (B, seq_len-1, vocab)\n        \n        # Reshape for loss\n        outputs = outputs.reshape(-1, outputs.size(-1))\n        targets = captions[:, 1:].reshape(-1)\n        \n        # Loss\n        loss = criterion(outputs, targets)\n        \n        # Backward\n        optimizer.zero_grad()\n        loss.backward()\n        \n        # Gradient clipping\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        \n        optimizer.step()\n        scheduler.step()\n        \n        total_loss += loss.item()\n        \n        # Update progress bar\n        current_lr = optimizer.param_groups[0]['lr']\n        loop.set_postfix(loss=loss.item(), lr=current_lr)\n    \n    avg_loss = total_loss / len(train_loader)\n    return avg_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T15:53:07.754716Z","iopub.execute_input":"2026-02-10T15:53:07.755343Z","iopub.status.idle":"2026-02-10T15:53:07.761480Z","shell.execute_reply.started":"2026-02-10T15:53:07.755314Z","shell.execute_reply":"2026-02-10T15:53:07.760624Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def validate(model, val_loader, criterion, epoch):\n    \"\"\"Validation\"\"\"\n    model.eval()\n    total_loss = 0\n    \n    with torch.no_grad():\n        loop = tqdm(val_loader, desc=f\"Epoch {epoch}/{NUM_EPOCHS} [VAL]\", leave=True)\n        \n        for features, captions in loop:\n            features = features.to(device)\n            captions = captions.to(device)\n            \n            # Forward\n            outputs = model(features, captions[:, :-1])\n            \n            # Reshape\n            outputs = outputs.reshape(-1, outputs.size(-1))\n            targets = captions[:, 1:].reshape(-1)\n            \n            # Loss\n            loss = criterion(outputs, targets)\n            total_loss += loss.item()\n            \n            loop.set_postfix(loss=loss.item())\n    \n    avg_loss = total_loss / len(val_loader)\n    return avg_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T15:53:08.937065Z","iopub.execute_input":"2026-02-10T15:53:08.937380Z","iopub.status.idle":"2026-02-10T15:53:08.942978Z","shell.execute_reply.started":"2026-02-10T15:53:08.937352Z","shell.execute_reply":"2026-02-10T15:53:08.942339Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"import torch\nimport wandb\n\n# --- C·∫§U H√åNH WANDB ---\n# 1. B·∫°n v√†o Kaggle -> Add-ons -> Secrets -> Add new secret\n# Label: wandb_key, Value: (L·∫•y API Key t·ª´ https://wandb.ai/authorize)\nfrom kaggle_secrets import UserSecretsClient\n\ntry:\n    user_secrets = UserSecretsClient()\n    wandb_api_key = user_secrets.get_secret(\"wandb_key\")\n    wandb.login(key=wandb_api_key)\nexcept:\n    # N·∫øu ch∆∞a set secret th√¨ nh·∫≠p tay khi ch·∫°y\n    wandb.login() \n\n# Kh·ªüi t·∫°o project (N√≥ s·∫Ω t·ª± t·∫°o project m·ªõi tr√™n web/app cho b·∫°n)\nrun = wandb.init(\n    project=\"transformer-project\", \n    name=\"kaggle_run_mobile_tracking\",\n    config={\n        \"learning_rate\": optimizer.param_groups[0]['lr'],\n        \"epochs\": NUM_EPOCHS,\n        \"batch_size\": 32, # V√≠ d·ª•, b·∫°n c√≥ th·ªÉ thay s·ªë th·ª±c t·∫ø\n        \"architecture\": \"Transformer\"\n    }\n)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"STARTING TRAINING\")\nprint(\"=\"*80)\n\nbest_val_loss = float('inf')\npatience_counter = 0\n\nfor epoch in range(1, NUM_EPOCHS + 1):\n    print(f\"\\n{'='*80}\")\n    print(f\"EPOCH {epoch}/{NUM_EPOCHS}\")\n    print('='*80)\n    \n    # Train\n    train_loss = train_one_epoch(model, train_loader, criterion, optimizer, scheduler, epoch)\n    \n    # Validate\n    val_loss = validate(model, val_loader, criterion, epoch)\n    \n    current_lr = optimizer.param_groups[0]['lr']\n\n    print(f\"\\nEpoch {epoch} Results:\")\n    print(f\"Train Loss: {train_loss:.4f}\")\n    print(f\"Val Loss:   {val_loss:.4f}\")\n    \n    # --- LOG L√äN WANDB (Quan tr·ªçng nh·∫•t) ---\n    # D√≤ng n√†y s·∫Ω ƒë·∫©y d·ªØ li·ªáu l√™n app ƒëi·ªán tho·∫°i ngay l·∫≠p t·ª©c\n    wandb.log({\n        \"epoch\": epoch,\n        \"train_loss\": train_loss,\n        \"val_loss\": val_loss,\n        \"learning_rate\": current_lr\n    })\n\n    # Save best model\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        patience_counter = 0\n        torch.save(model.state_dict(), \"best_model.pth\")\n        print(f\"Saved best model (Val Loss: {best_val_loss:.4f})\")\n    else:\n        patience_counter += 1\n        print(f\"No improvement ({patience_counter}/{EARLY_STOP_PATIENCE})\")\n    \n    # Early stopping\n    if patience_counter >= EARLY_STOP_PATIENCE:\n        print(f\"\\nEarly stopping at epoch {epoch}\")\n        break\n\n# K·∫øt th√∫c tracking\nwandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T13:28:06.966162Z","iopub.execute_input":"2026-02-10T13:28:06.966779Z","iopub.status.idle":"2026-02-10T14:07:11.365774Z","shell.execute_reply.started":"2026-02-10T13:28:06.966752Z","shell.execute_reply":"2026-02-10T14:07:11.364907Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n  | |_| | '_ \\/ _` / _` |  _/ -_)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mleducminh583\u001b[0m (\u001b[33mleducminh583-national-economics-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.22.2"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20260210_132817-memulz7j</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/leducminh583-national-economics-university/transformer-project/runs/memulz7j' target=\"_blank\">kaggle_run_mobile_tracking</a></strong> to <a href='https://wandb.ai/leducminh583-national-economics-university/transformer-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/leducminh583-national-economics-university/transformer-project' target=\"_blank\">https://wandb.ai/leducminh583-national-economics-university/transformer-project</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/leducminh583-national-economics-university/transformer-project/runs/memulz7j' target=\"_blank\">https://wandb.ai/leducminh583-national-economics-university/transformer-project/runs/memulz7j</a>"},"metadata":{}},{"name":"stdout","text":"\n================================================================================\nSTARTING TRAINING\n================================================================================\n\n================================================================================\nEPOCH 1/15\n================================================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/15 [TRAIN]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4470/4470 [02:28<00:00, 30.12it/s, loss=3.64, lr=7.05e-5]\nEpoch 1/15 [VAL]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 497/497 [00:07<00:00, 67.37it/s, loss=3.3] \n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1 Results:\nTrain Loss: 3.7352\nVal Loss:   3.7137\nSaved best model (Val Loss: 3.7137)\n\n================================================================================\nEPOCH 2/15\n================================================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/15 [TRAIN]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4470/4470 [02:27<00:00, 30.33it/s, loss=3.58, lr=5.65e-5]\nEpoch 2/15 [VAL]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 497/497 [00:07<00:00, 65.10it/s, loss=3.21]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2 Results:\nTrain Loss: 3.5145\nVal Loss:   3.5965\nSaved best model (Val Loss: 3.5965)\n\n================================================================================\nEPOCH 3/15\n================================================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/15 [TRAIN]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4470/4470 [02:28<00:00, 30.19it/s, loss=3.04, lr=4.85e-5]\nEpoch 3/15 [VAL]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 497/497 [00:07<00:00, 67.54it/s, loss=3.23]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 3 Results:\nTrain Loss: 3.4171\nVal Loss:   3.5583\nSaved best model (Val Loss: 3.5583)\n\n================================================================================\nEPOCH 4/15\n================================================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/15 [TRAIN]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4470/4470 [02:27<00:00, 30.27it/s, loss=3.02, lr=4.32e-5]\nEpoch 4/15 [VAL]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 497/497 [00:07<00:00, 66.90it/s, loss=3.18]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 4 Results:\nTrain Loss: 3.3541\nVal Loss:   3.5143\nSaved best model (Val Loss: 3.5143)\n\n================================================================================\nEPOCH 5/15\n================================================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/15 [TRAIN]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4470/4470 [02:27<00:00, 30.32it/s, loss=3.08, lr=3.93e-5]\nEpoch 5/15 [VAL]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 497/497 [00:07<00:00, 67.14it/s, loss=3.21]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 5 Results:\nTrain Loss: 3.3060\nVal Loss:   3.4815\nSaved best model (Val Loss: 3.4815)\n\n================================================================================\nEPOCH 6/15\n================================================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/15 [TRAIN]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4470/4470 [02:27<00:00, 30.37it/s, loss=3.31, lr=3.63e-5]\nEpoch 6/15 [VAL]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 497/497 [00:07<00:00, 65.41it/s, loss=3.18]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 6 Results:\nTrain Loss: 3.2675\nVal Loss:   3.4673\nSaved best model (Val Loss: 3.4673)\n\n================================================================================\nEPOCH 7/15\n================================================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/15 [TRAIN]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4470/4470 [02:27<00:00, 30.39it/s, loss=3.39, lr=3.39e-5]\nEpoch 7/15 [VAL]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 497/497 [00:07<00:00, 68.09it/s, loss=3.2] \n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 7 Results:\nTrain Loss: 3.2338\nVal Loss:   3.4393\nSaved best model (Val Loss: 3.4393)\n\n================================================================================\nEPOCH 8/15\n================================================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/15 [TRAIN]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4470/4470 [02:28<00:00, 30.19it/s, loss=3.36, lr=3.19e-5]\nEpoch 8/15 [VAL]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 497/497 [00:07<00:00, 67.14it/s, loss=3.2] \n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 8 Results:\nTrain Loss: 3.2027\nVal Loss:   3.4376\nSaved best model (Val Loss: 3.4376)\n\n================================================================================\nEPOCH 9/15\n================================================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/15 [TRAIN]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4470/4470 [02:27<00:00, 30.30it/s, loss=2.72, lr=3.02e-5]\nEpoch 9/15 [VAL]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 497/497 [00:07<00:00, 67.67it/s, loss=3.19]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 9 Results:\nTrain Loss: 3.1775\nVal Loss:   3.4303\nSaved best model (Val Loss: 3.4303)\n\n================================================================================\nEPOCH 10/15\n================================================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/15 [TRAIN]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4470/4470 [02:27<00:00, 30.40it/s, loss=3.1, lr=2.88e-5] \nEpoch 10/15 [VAL]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 497/497 [00:07<00:00, 66.83it/s, loss=3.18]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 10 Results:\nTrain Loss: 3.1541\nVal Loss:   3.4143\nSaved best model (Val Loss: 3.4143)\n\n================================================================================\nEPOCH 11/15\n================================================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/15 [TRAIN]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4470/4470 [02:27<00:00, 30.35it/s, loss=2.58, lr=2.75e-5]\nEpoch 11/15 [VAL]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 497/497 [00:07<00:00, 67.19it/s, loss=3.15]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 11 Results:\nTrain Loss: 3.1308\nVal Loss:   3.4044\nSaved best model (Val Loss: 3.4044)\n\n================================================================================\nEPOCH 12/15\n================================================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/15 [TRAIN]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4470/4470 [02:27<00:00, 30.38it/s, loss=3.29, lr=2.64e-5]\nEpoch 12/15 [VAL]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 497/497 [00:07<00:00, 66.63it/s, loss=3.17]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 12 Results:\nTrain Loss: 3.1121\nVal Loss:   3.3969\nSaved best model (Val Loss: 3.3969)\n\n================================================================================\nEPOCH 13/15\n================================================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/15 [TRAIN]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4470/4470 [02:27<00:00, 30.31it/s, loss=2.8, lr=2.55e-5] \nEpoch 13/15 [VAL]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 497/497 [00:07<00:00, 68.17it/s, loss=3.2] \n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 13 Results:\nTrain Loss: 3.0932\nVal Loss:   3.3869\nSaved best model (Val Loss: 3.3869)\n\n================================================================================\nEPOCH 14/15\n================================================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/15 [TRAIN]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4470/4470 [02:26<00:00, 30.43it/s, loss=3.5, lr=2.46e-5] \nEpoch 14/15 [VAL]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 497/497 [00:07<00:00, 67.26it/s, loss=3.16]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 14 Results:\nTrain Loss: 3.0761\nVal Loss:   3.3909\nNo improvement (1/5)\n\n================================================================================\nEPOCH 15/15\n================================================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/15 [TRAIN]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4470/4470 [02:27<00:00, 30.29it/s, loss=3.1, lr=2.38e-5] \nEpoch 15/15 [VAL]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 497/497 [00:07<00:00, 66.70it/s, loss=3.21]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 15 Results:\nTrain Loss: 3.0592\nVal Loss:   3.3865\nSaved best model (Val Loss: 3.3865)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà</td></tr><tr><td>learning_rate</td><td>‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train_loss</td><td>‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_loss</td><td>‚ñà‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>learning_rate</td><td>2e-05</td></tr><tr><td>train_loss</td><td>3.05923</td></tr><tr><td>val_loss</td><td>3.38654</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">kaggle_run_mobile_tracking</strong> at: <a href='https://wandb.ai/leducminh583-national-economics-university/transformer-project/runs/memulz7j' target=\"_blank\">https://wandb.ai/leducminh583-national-economics-university/transformer-project/runs/memulz7j</a><br> View project at: <a href='https://wandb.ai/leducminh583-national-economics-university/transformer-project' target=\"_blank\">https://wandb.ai/leducminh583-national-economics-university/transformer-project</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20260210_132817-memulz7j/logs</code>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"def generate_caption_beam_search(model, img_path, vocab, encoder, transform,\n                                 max_len=20, beam_width=3):\n    \"\"\"\n    Generate caption v·ªõi beam search\n    \n    Args:\n        model: Trained Transformer model\n        img_path: Path to image\n        vocab: Vocabulary object\n        encoder: CNN encoder\n        transform: Image transform\n        max_len: Max caption length\n        beam_width: Beam width\n    \n    Returns:\n        caption: Generated caption string\n    \"\"\"\n    model.eval()\n    device = next(model.parameters()).device\n    \n    # Load v√† preprocess image\n    img = Image.open(img_path).convert(\"RGB\")\n    img_tensor = transform(img).unsqueeze(0).to(device)\n    \n    # Extract features\n    with torch.no_grad():\n        features = encoder(img_tensor)  # (1, 49, 1280)\n    \n    # Initialize beam\n    sequences = [[vocab.stoi[\"<start>\"]]]\n    scores = [0.0]\n    \n    for step in range(max_len):\n        all_candidates = []\n        \n        for i, seq in enumerate(sequences):\n            # N·∫øu sequence ƒë√£ k·∫øt th√∫c\n            if seq[-1] == vocab.stoi[\"<end>\"]:\n                all_candidates.append((seq, scores[i]))\n                continue\n            \n            # Prepare input\n            tgt = torch.LongTensor([seq]).to(device)  # (1, len(seq))\n            \n            # Forward pass\n            with torch.no_grad():\n                logits = model(features, tgt)  # (1, len(seq), vocab_size)\n                log_probs = torch.log_softmax(logits[:, -1, :], dim=-1)  # (1, vocab_size)\n            \n            # Get top-k\n            topk_log_probs, topk_indices = torch.topk(log_probs, beam_width)\n            \n            for j in range(beam_width):\n                candidate_seq = seq + [topk_indices[0][j].item()]\n                candidate_score = scores[i] + topk_log_probs[0][j].item()\n                all_candidates.append((candidate_seq, candidate_score))\n        \n        # Keep top beam_width sequences\n        ordered = sorted(all_candidates, key=lambda x: x[1], reverse=True)\n        sequences = [seq for seq, _ in ordered[:beam_width]]\n        scores = [score for _, score in ordered[:beam_width]]\n        \n        # Early stop\n        if all(seq[-1] == vocab.stoi[\"<end>\"] for seq in sequences):\n            break\n    \n    # Return best sequence\n    best_seq = sequences[0]\n    caption = [vocab.itos[idx] for idx in best_seq[1:] \n               if idx not in [vocab.stoi[\"<end>\"], vocab.stoi[\"<pad>\"]]]\n    \n    return ' '.join(caption)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T15:53:14.814896Z","iopub.execute_input":"2026-02-10T15:53:14.815203Z","iopub.status.idle":"2026-02-10T15:53:14.825097Z","shell.execute_reply.started":"2026-02-10T15:53:14.815176Z","shell.execute_reply":"2026-02-10T15:53:14.824196Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"def evaluate_bleu(model, df_eval, vocab, encoder, transform, num_samples=500):\n    \"\"\"\n    ƒê√°nh gi√° BLEU score\n    \"\"\"\n    from nltk.translate.bleu_score import corpus_bleu\n    \n    print(\"\\n\" + \"=\"*80)\n    print(f\"üìä EVALUATING BLEU SCORE (on {num_samples} samples)\")\n    print(\"=\"*80)\n    \n    # Group captions by image\n    grouped_df = df_eval.groupby('image_name')['comment'].apply(list).reset_index()\n    \n    if num_samples < len(grouped_df):\n        sample_df = grouped_df.sample(n=num_samples, random_state=42)\n    else:\n        sample_df = grouped_df\n    \n    references = []\n    hypotheses = []\n    \n    for idx, row in tqdm(sample_df.iterrows(), total=len(sample_df), desc=\"Generating captions\"):\n        img_name = row['image_name']\n        true_captions = row['comment']\n        \n        img_path = os.path.join(IMG_DIR, img_name)\n        \n        try:\n            # Generate caption\n            pred_caption = generate_caption_beam_search(\n                model, img_path, vocab, encoder, transform, \n                max_len=20, beam_width=3\n            )\n            \n            pred_tokens = pred_caption.split()\n            hypotheses.append(pred_tokens)\n            \n            # Tokenize ground truth captions\n            refs_tokens = [vocab.tokenizer_eng(c) for c in true_captions]\n            references.append(refs_tokens)\n            \n        except Exception as e:\n            print(f\"Error with {img_name}: {e}\")\n            continue\n    \n    # Calculate BLEU\n    bleu1 = corpus_bleu(references, hypotheses, weights=(1, 0, 0, 0))\n    bleu2 = corpus_bleu(references, hypotheses, weights=(0.5, 0.5, 0, 0))\n    bleu3 = corpus_bleu(references, hypotheses, weights=(0.33, 0.33, 0.33, 0))\n    bleu4 = corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25))\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"üìä BLEU SCORE RESULTS\")\n    print(\"=\"*80)\n    print(f\"BLEU-1: {bleu1:.4f}\")\n    print(f\"BLEU-2: {bleu2:.4f}\")\n    print(f\"BLEU-3: {bleu3:.4f}\")\n    print(f\"BLEU-4: {bleu4:.4f}\")\n    print(\"=\"*80)\n    \n    return bleu1, bleu2, bleu3, bleu4\n\n\n# Evaluate model\nprint(\"\\nüîÑ Loading best model for evaluation...\")\nmodel.load_state_dict(torch.load(\"/kaggle/working/best_model.pth\"))\nmodel.eval()\n\nevaluate_bleu(model, df, vocab, encoder, transform, num_samples=500)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T15:53:16.900834Z","iopub.execute_input":"2026-02-10T15:53:16.901098Z","iopub.status.idle":"2026-02-10T15:54:33.662046Z","shell.execute_reply.started":"2026-02-10T15:53:16.901077Z","shell.execute_reply":"2026-02-10T15:54:33.661455Z"}},"outputs":[{"name":"stdout","text":"\nüîÑ Loading best model for evaluation...\n\n================================================================================\nüìä EVALUATING BLEU SCORE (on 500 samples)\n================================================================================\n","output_type":"stream"},{"name":"stderr","text":"Generating captions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [01:14<00:00,  6.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nüìä BLEU SCORE RESULTS\n================================================================================\nBLEU-1: 0.6852\nBLEU-2: 0.5220\nBLEU-3: 0.3975\nBLEU-4: 0.2942\n================================================================================\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"(0.6852167329169033,\n 0.5219982485048974,\n 0.3974973270137489,\n 0.29419529466015876)"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"print(\"\\n\" + \"=\"*80)\nprint(\"TESTING WITH SAMPLE IMAGES\")\nprint(\"=\"*80)\n\n# L·∫•y 5 ·∫£nh ng·∫´u nhi√™n\nsample_images = df['image_name'].unique()[:5]\n\nfor img_name in sample_images:\n    img_path = os.path.join(IMG_DIR, img_name)\n    \n    # Ground truth captions\n    true_captions = df[df['image_name'] == img_name]['comment'].values\n    \n    # Generated caption\n    try:\n        pred_caption = generate_caption_beam_search(\n            model, img_path, vocab, encoder, transform,\n            max_len=20, beam_width=3\n        )\n        \n        print(f\"\\nImage: {img_name}\")\n        print(f\"Generated: {pred_caption}\")\n        print(f\"Ground Truth:\")\n        for i, cap in enumerate(true_captions[:3]):  # Show first 3\n            print(f\"   {i+1}. {cap}\")\n        print(\"-\" * 80)\n        \n    except Exception as e:\n        print(f\"Error with {img_name}: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T15:57:09.619145Z","iopub.execute_input":"2026-02-10T15:57:09.619964Z","iopub.status.idle":"2026-02-10T15:57:10.355483Z","shell.execute_reply.started":"2026-02-10T15:57:09.619932Z","shell.execute_reply":"2026-02-10T15:57:10.354825Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nTESTING WITH SAMPLE IMAGES\n================================================================================\n\nImage: 1000092795.jpg\nGenerated: a man in a green shirt and jeans is standing in front of a tree\nGround Truth:\n   1. Two young guys with shaggy hair look at their hands while hanging out in the yard .\n   2. Two young  White males are outside near many bushes .\n   3. Two men in green shirts are standing in a yard .\n--------------------------------------------------------------------------------\n\nImage: 10002456.jpg\nGenerated: two men are working on a machine\nGround Truth:\n   1. Several men in hard hats are operating a giant pulley system .\n   2. Workers look down from up above on a piece of equipment .\n   3. Two men working on a machine wearing hard hats .\n--------------------------------------------------------------------------------\n\nImage: 1000268201.jpg\nGenerated: a little girl in a pink dress is sitting on a wooden bench\nGround Truth:\n   1. A child in a pink dress is climbing up a set of stairs in an entry way .\n   2. A little girl in a pink dress going into a wooden cabin .\n   3. A little girl climbing the stairs to her playhouse .\n--------------------------------------------------------------------------------\n\nImage: 1000344755.jpg\nGenerated: a man on a ladder painting a building\nGround Truth:\n   1. Someone in a blue shirt and hat is standing on stair and leaning against a window .\n   2. A man in a blue shirt is standing on a ladder cleaning a window .\n   3. A man on a ladder cleans the window of a tall building .\n--------------------------------------------------------------------------------\n\nImage: 1000366164.jpg\nGenerated: two men cooking in a kitchen\nGround Truth:\n   1. Two men  one in a gray shirt  one in a black shirt  standing near a stove .\n   2. Two guy cooking and joking around with the camera .\n   3. Two men in a kitchen cooking food on a stove .\n--------------------------------------------------------------------------------\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"import pickle\nimport re\nfrom IPython.display import FileLink\n\n# ============================================================================\n# ƒê·ªäNH NGHƒ®A L·∫†I CLASS (B·∫ÆT BU·ªòC)\n# ============================================================================\n\nclass Vocabulary:\n    \"\"\"\n    X√¢y d·ª±ng t·ª´ ƒëi·ªÉn t·ª´ captions\n    \"\"\"\n    def __init__(self, freq_threshold):\n        self.itos = {0: \"<pad>\", 1: \"<start>\", 2: \"<end>\", 3: \"<unk>\"}\n        self.stoi = {\"<pad>\": 0, \"<start>\": 1, \"<end>\": 2, \"<unk>\": 3}\n        self.freq_threshold = freq_threshold\n    \n    def __len__(self):\n        return len(self.itos)\n    \n    @staticmethod\n    def tokenizer_eng(text):\n        \"\"\"\n        Tokenize English text\n        FIX: S·ª≠ d·ª•ng regex ƒë·ªÉ lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát\n        \"\"\"\n        text = str(text).lower()\n        text = re.sub(r'[^a-z ]', '', text)  # Ch·ªâ gi·ªØ ch·ªØ c√°i v√† kho·∫£ng tr·∫Øng\n        return text.split()\n    \n    def build_vocabulary(self, sentence_list):\n        \"\"\"X√¢y d·ª±ng vocabulary t·ª´ list captions\"\"\"\n        frequencies = {}\n        idx = 4\n        for sentence in sentence_list:\n            for word in self.tokenizer_eng(sentence):\n                frequencies[word] = frequencies.get(word, 0) + 1\n                if frequencies[word] == self.freq_threshold:\n                    self.stoi[word] = idx\n                    self.itos[idx] = word\n                    idx += 1\n    \n    def numericalize(self, text):\n        \"\"\"Chuy·ªÉn text th√†nh list of indices\"\"\"\n        tokenized_text = self.tokenizer_eng(text)\n        return [self.stoi.get(token, self.stoi[\"<unk>\"]) for token in tokenized_text]\n\n\n# ============================================================================\n# SAVE VOCABULARY\n# ============================================================================\n\nprint(\"=\"*80)\nprint(\"üíæ SAVING VOCABULARY FOR DEPLOYMENT\")\nprint(\"=\"*80)\n\n# Ki·ªÉm tra vocab ƒë√£ t·ªìn t·∫°i ch∆∞a\ntry:\n    print(f\"‚úÖ Found existing vocab with {len(vocab)} tokens\")\nexcept:\n    print(\"‚ùå Vocab ch∆∞a t·ªìn t·∫°i! C·∫ßn train model tr∆∞·ªõc.\")\n    raise\n\n# Save vocabulary object\nwith open('/kaggle/working/vocab.pkl', 'wb') as f:\n    pickle.dump(vocab, f)\n\nprint(f\"‚úÖ Vocabulary saved successfully!\")\nprint(f\"   File: /kaggle/working/vocab.pkl\")\nprint(f\"   Size: {len(vocab)} tokens\")\nprint(f\"   Freq threshold: {vocab.freq_threshold}\")\nprint(f\"\\n   Special tokens:\")\nprint(f\"      <pad>:   {vocab.stoi['<pad>']}\")\nprint(f\"      <start>: {vocab.stoi['<start>']}\")\nprint(f\"      <end>:   {vocab.stoi['<end>']}\")\nprint(f\"      <unk>:   {vocab.stoi['<unk>']}\")\n\n# Test load l·∫°i ƒë·ªÉ ƒë·∫£m b·∫£o kh√¥ng l·ªói\nprint(\"\\nüîÑ Testing load...\")\nwith open('/kaggle/working/vocab.pkl', 'rb') as f:\n    test_vocab = pickle.load(f)\n\nprint(f\"‚úÖ Load test successful! Vocab size: {len(test_vocab)}\")\n\n# Test tokenizer\ntest_sentence = \"A dog is running in the park.\"\ntokens = test_vocab.tokenizer_eng(test_sentence)\nindices = test_vocab.numericalize(test_sentence)\nprint(f\"\\nüß™ Tokenizer test:\")\nprint(f\"   Input:   '{test_sentence}'\")\nprint(f\"   Tokens:  {tokens}\")\nprint(f\"   Indices: {indices[:10]}...\")  # Show first 10\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"üì• DOWNLOAD FILES\")\nprint(\"=\"*80)\nprint(\"Click v√†o c√°c link ƒë·ªÉ t·∫£i xu·ªëng:\")\nprint()\n\n# Model weights\nprint(\"1Ô∏è‚É£ Model weights (~100MB):\")\ndisplay(FileLink('/kaggle/working/best_transformer_model.pth'))\n\n# Vocabulary\nprint(\"\\n2Ô∏è‚É£ Vocabulary (~1MB):\")\ndisplay(FileLink('/kaggle/working/vocab.pkl'))\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"‚úÖ FILES READY FOR DEPLOYMENT!\")\nprint(\"=\"*80)\nprint(\"\\nüìã Next steps:\")\nprint(\"1. ‚úÖ Download c·∫£ 2 files (click links tr√™n)\")\nprint(\"2. üåê Go to https://huggingface.co/spaces\")\nprint(\"3. ‚ûï Create new Space (SDK: Gradio)\")\nprint(\"4. üì§ Upload 6 files:\")\nprint(\"      - app.py\")\nprint(\"      - model.py\")\nprint(\"      - requirements.txt\")\nprint(\"      - README.md\")\nprint(\"      - best_transformer_model.pth (t·ª´ Kaggle)\")\nprint(\"      - vocab.pkl (t·ª´ Kaggle)\")\nprint(\"5. ‚è≥ Wait for build (~5 minutes)\")\nprint(\"6. üéâ Your app is LIVE!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T03:27:30.751171Z","iopub.execute_input":"2026-02-11T03:27:30.751790Z","iopub.status.idle":"2026-02-11T03:27:30.774907Z","shell.execute_reply.started":"2026-02-11T03:27:30.751743Z","shell.execute_reply":"2026-02-11T03:27:30.773429Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nüíæ SAVING VOCABULARY FOR DEPLOYMENT\n================================================================================\n‚ùå Vocab ch∆∞a t·ªìn t·∫°i! C·∫ßn train model tr∆∞·ªõc.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/2412625994.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;31m# Ki·ªÉm tra vocab ƒë√£ t·ªìn t·∫°i ch∆∞a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"‚úÖ Found existing vocab with {len(vocab)} tokens\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"‚ùå Vocab ch∆∞a t·ªìn t·∫°i! C·∫ßn train model tr∆∞·ªõc.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'vocab' is not defined"],"ename":"NameError","evalue":"name 'vocab' is not defined","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"import os\n\n# ƒê∆∞·ªùng d·∫´n c·∫ßn ki·ªÉm tra\nWORK_DIR = \"/kaggle/working\"\n\nprint(f\"üìÇ ƒêang li·ªát k√™ n·ªôi dung trong: {WORK_DIR}\\n\")\n\n# Duy·ªát qua c√¢y th∆∞ m·ª•c\nfor root, dirs, files in os.walk(WORK_DIR):\n    # T√≠nh c·∫•p ƒë·ªô s√¢u ƒë·ªÉ in th·ª•t l·ªÅ cho ƒë·∫πp\n    level = root.replace(WORK_DIR, '').count(os.sep)\n    indent = ' ' * 4 * (level)\n    print(f\"{indent}üìÅ {os.path.basename(root)}/\")\n    subindent = ' ' * 4 * (level + 1)\n    \n    # In ra t·ªëi ƒëa 5 file ƒë·∫ßu ti√™n trong m·ªói th∆∞ m·ª•c ƒë·ªÉ tr√°nh b·ªã spam m√†n h√¨nh\n    for f in files[:5]:\n        print(f\"{subindent}üìÑ {f}\")\n    \n    if len(files) > 5:\n        print(f\"{subindent}... (c√≤n {len(files) - 5} file n·ªØa)\")\n\nprint(\"\\n‚úÖ Ho√†n t·∫•t li·ªát k√™.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\nimport re\nfrom IPython.display import FileLink\n\n# ============================================================================\n# ƒê·ªäNH NGHƒ®A CLASS VOCABULARY (B·∫ÆT BU·ªòC)\n# ============================================================================\n\nclass Vocabulary:\n    def __init__(self, freq_threshold):\n        self.itos = {0: \"<pad>\", 1: \"<start>\", 2: \"<end>\", 3: \"<unk>\"}\n        self.stoi = {\"<pad>\": 0, \"<start>\": 1, \"<end>\": 2, \"<unk>\": 3}\n        self.freq_threshold = freq_threshold\n    \n    def __len__(self):\n        return len(self.itos)\n    \n    @staticmethod\n    def tokenizer_eng(text):\n        text = str(text).lower()\n        text = re.sub(r'[^a-z ]', '', text)\n        return text.split()\n    \n    def build_vocabulary(self, sentence_list):\n        frequencies = {}\n        idx = 4\n        for sentence in sentence_list:\n            for word in self.tokenizer_eng(sentence):\n                frequencies[word] = frequencies.get(word, 0) + 1\n                if frequencies[word] == self.freq_threshold:\n                    self.stoi[word] = idx\n                    self.itos[idx] = word\n                    idx += 1\n    \n    def numericalize(self, text):\n        tokenized_text = self.tokenizer_eng(text)\n        return [self.stoi.get(token, self.stoi[\"<unk>\"]) for token in tokenized_text]\n\n\n# ============================================================================\n# KI·ªÇM TRA VOCAB T·ªíN T·∫†I\n# ============================================================================\n\n\n\n# ============================================================================\n# SAVE VOCABULARY\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"üíæ SAVING VOCABULARY\")\nprint(\"=\"*80)\n\nwith open('/kaggle/working/vocab.pkl', 'wb') as f:\n    pickle.dump(existing_vocab, f)\n\nprint(\"‚úÖ Saved vocab.pkl\")\n\n# Test load\nwith open('/kaggle/working/vocab.pkl', 'rb') as f:\n    test_vocab = pickle.load(f)\n\nprint(f\"‚úÖ Test load successful: {len(test_vocab)} tokens\")\n\n# Test tokenizer\ntest_text = \"A dog is running.\"\ntokens = test_vocab.tokenizer_eng(test_text)\nprint(f\"‚úÖ Test tokenize: '{test_text}' ‚Üí {tokens}\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"üì• DOWNLOAD VOCAB\")\nprint(\"=\"*80)\ndisplay(FileLink('/kaggle/working/vocab.pkl'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T16:26:37.136891Z","iopub.execute_input":"2026-02-11T16:26:37.137329Z","iopub.status.idle":"2026-02-11T16:26:37.156697Z","shell.execute_reply.started":"2026-02-11T16:26:37.137303Z","shell.execute_reply":"2026-02-11T16:26:37.155307Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Found existing vocab with 9964 tokens\n\n================================================================================\nüíæ SAVING VOCABULARY\n================================================================================\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/3724890875.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/kaggle/working/vocab.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexisting_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"‚úÖ Saved vocab.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mPicklingError\u001b[0m: Can't pickle <class '__main__.Vocabulary'>: it's not the same object as __main__.Vocabulary"],"ename":"PicklingError","evalue":"Can't pickle <class '__main__.Vocabulary'>: it's not the same object as __main__.Vocabulary","output_type":"error"}],"execution_count":2},{"cell_type":"code","source":"\ntry:\n    # N·∫øu vocab ƒë√£ c√≥ s·∫µn trong notebook\n    print(f\"‚úÖ Found existing vocab with {len(vocab)} tokens\")\n    existing_vocab = vocab\nexcept NameError:\n    # N·∫øu ch∆∞a c√≥ vocab ‚Üí t·∫°o m·ªõi\n    print(\"‚ö†Ô∏è  Vocab ch∆∞a t·ªìn t·∫°i, ƒëang t·∫°o m·ªõi...\")\n    \n    import pandas as pd\n    df = pd.read_csv(\"/kaggle/input/flickr30k/captions.txt\")\n    \n    existing_vocab = Vocabulary(freq_threshold=3)\n    existing_vocab.build_vocabulary(df['comment'].tolist())\n    \n    print(f\"‚úÖ Created new vocab with {len(existing_vocab)} tokens\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T16:27:04.822512Z","iopub.execute_input":"2026-02-11T16:27:04.822899Z","iopub.status.idle":"2026-02-11T16:27:04.830760Z","shell.execute_reply.started":"2026-02-11T16:27:04.822873Z","shell.execute_reply":"2026-02-11T16:27:04.829139Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Found existing vocab with 9964 tokens\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"print(\"\\n\" + \"=\"*80)\nprint(\"üíæ SAVING VOCABULARY\")\nprint(\"=\"*80)\n\nwith open('/kaggle/working/vocab.pkl', 'wb') as f:\n    pickle.dump(existing_vocab, f)\n\nprint(\"‚úÖ Saved vocab.pkl\")\n\n# Test load\nwith open('/kaggle/working/vocab.pkl', 'rb') as f:\n    test_vocab = pickle.load(f)\n\nprint(f\"‚úÖ Test load successful: {len(test_vocab)} tokens\")\n\n# Test tokenizer\ntest_text = \"A dog is running.\"\ntokens = test_vocab.tokenizer_eng(test_text)\nprint(f\"‚úÖ Test tokenize: '{test_text}' ‚Üí {tokens}\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"üì• DOWNLOAD VOCAB\")\nprint(\"=\"*80)\ndisplay(FileLink('/kaggle/working/vocab.pkl'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T16:27:06.001320Z","iopub.execute_input":"2026-02-11T16:27:06.003040Z","iopub.status.idle":"2026-02-11T16:27:06.016487Z","shell.execute_reply.started":"2026-02-11T16:27:06.002962Z","shell.execute_reply":"2026-02-11T16:27:06.014445Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nüíæ SAVING VOCABULARY\n================================================================================\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/1250989499.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/kaggle/working/vocab.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexisting_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"‚úÖ Saved vocab.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mPicklingError\u001b[0m: Can't pickle <class '__main__.Vocabulary'>: it's not the same object as __main__.Vocabulary"],"ename":"PicklingError","evalue":"Can't pickle <class '__main__.Vocabulary'>: it's not the same object as __main__.Vocabulary","output_type":"error"}],"execution_count":4},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}